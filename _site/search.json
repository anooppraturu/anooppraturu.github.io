[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Anoop Praturu, I’m a AI researcher at Unlearn.ai interested in Bayesian Inference, Statistical Physics, and Interpretability. I got my PhD in Physics from UCSD under the supervision of Tatyana Sharpee, where I used techniques from differential geometry and statistical mechanics to study problems in neuroscience and biophysics. Before that I was a Data Scientist at Invoca where I researched non-parametric Bayesian models and MCMC theory for NLP. I got my B.S. in Physics at UCSB in the College of Creative Studies.\nThis site is a collection of technical notes and essays on topics that interest me. The title of this blog is from a footnote in Schrödinger’s paper “Quantization as a Boundary Value Problem”. If you have any thoughts on my writing, feel free to get in touch!\n\n\n\n Email: anooppraturu [at] gmail [dot] com\n GitHub: https://github.com/anooppraturu\n Google Scholar: https://scholar.google.com/citations?user=EMp1gWwAAAAJ&hl=en&oi=ao"
  },
  {
    "objectID": "posts/gp-regression/index.html",
    "href": "posts/gp-regression/index.html",
    "title": "Gaussian Process Regression",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n#The script GP.py can be found on my github: https://github.com/anooppraturu/gaussian_processes\nimport GP\n\nmpl.rcParams.update({\n    \"font.family\": \"serif\",\n    \"font.serif\": [\"Computer Modern Roman\", \"Times New Roman\", \"DejaVu Serif\"],\n    \"mathtext.fontset\": \"cm\",\n    \"xtick.labelsize\": 16,\n    \"ytick.labelsize\": 16,\n})\nRegression is the art of taking data, and finding the function whence they came. Consider for example the dataset shown below plotting the number of miles I have run each week over the last few months. Denoting the week by \\(x\\) and the number of miles ran by \\(y\\), we suppose that there is some function \\(y=f(x)\\) that determines the relationship between these quantities, of which we only see a few samples from. This function could perhaps be a training plan that I am following for a race. Knowing this function can help you interpret data: if you find a good function it can tell you something deeper about the “true” relationship between the variables, like perhaps the specifics of my training plan. It can also help you make predictions: maybe a friend wants to see how busy I’ll be running next week before they reach out to make plans. If they have a good function they can predict how many miles I’ll run next week and decide not to hang out with me :( An example of one possible function is shown in the middle panel. This function is “perfect” in the sense that it reproduces each of our data points exactly. This is actually not so good, because our data is never perfect. For example the GPS on my phone that tracks my mileage on runs has a finite precision, so the numbers reported are not actually the number of miles I ran. Or maybe I got lazy one week and ran fewer miles than I should have for my training plan. Then the number of miles \\(y\\) would be different than the number predicted by the “true” function \\(f(x)\\). To remedy this we usually assume that our data come from some function but are corrupted by noise: \\(y = f(x) + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)\\) are i.i.d normally distributed noise, and the variance \\(\\sigma_n^2\\) is a hyperparameter determining the strength of this noise (the \\(n\\) here stands for noise, to distinguish this from the plethora of other \\(\\sigma\\)’s that will arise). What this means in practice is that the function we seek doesn’t need to pass exactly through every data point, like the one shown on the right. Including noise is a good idea not only because it’s literally true, but we will see that it also has some appealing mathematical properties.\nCode\n# Initialize dataset\nmiles = torch.tensor([6.97, 0.0, 1.32, 10.86, 10.57, 4.26, 9.70, 10.41, 12.71, 19.11, 23.84], dtype=float)\nt = torch.arange(len(miles), dtype=float)\n# test times\nt_full = torch.arange(0,10.5,0.1, dtype=float)\n\n# 0 noise RBF example to interpolate the data\nex_RBF_GP = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_lengthscale=torch.tensor(0.0),\n        log_variance=torch.tensor(0.0)\n    ),\n    log_noise = torch.tensor(-torch.inf)\n)\nex_RBF_GP.condition(t,miles)\nrbf_mu = ex_RBF_GP.predict(t_full)\n\n# Linear model example that doesn't interpolate data\nex_lin_GP = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        m=1,\n        log_c=torch.tensor(0.0),\n        log_variance=torch.tensor(0.0)\n    ),\n    log_noise = torch.tensor(0.0)\n)\nex_lin_GP.condition(t, miles)\nlin_mu = ex_lin_GP.predict(t_full)\n\n# Plotting\nfig, ax = plt.subplots(1, 3, figsize=(15,4.5))\n\nax[0].scatter(t, miles, s=75, c='#66c2a5')\nax[1].scatter(t, miles, s=75, c='#66c2a5')\nax[1].plot(t_full, rbf_mu, lw=4, c='#8da0cb', zorder=0)\n\nax[2].scatter(t, miles, s=75, c='#66c2a5')\nax[2].plot(t_full, lin_mu, lw=4, c='#8da0cb', zorder=0)\n\nax[0].set_ylabel(\"Miles\", fontsize=15);\nax[0].set_xlabel(\"Time (Weeks)\", fontsize=15);\nax[1].set_xlabel(\"Time (Weeks)\", fontsize=15);\nax[2].set_xlabel(\"Time (Weeks)\", fontsize=15);\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Left: How many miles I ran per week over the last few months. Middle: One possible regressing function that perfectly interpolates the data. Right: Another possible function which doesn’t interpolate by allowing for noise.\nOur task is particularly hard because there are alot of functions to choose from. In fact the space of all possible functions is infinite dimensional! A crude but intuitive way to see this is to consider the Taylor Series expansion of a function: \\[\nf(x) = \\sum_{n=0}^\\infty a_n x^n\n\\]\nEach candidate function can then be characterized by its list of Taylor coefficients \\(f \\leftrightarrow (a_0, a_1, a_2, \\ldots)\\). This infinite list of numbers is like a vector in an infinite dimenisonal space. Gaussian Process Regression is a framework that provides just enough structure to allow us to make a foray into function space in our search for good functions. The central tool in this framework is something called a Gaussian Process, which is a natural generalization of the multivariate Gaussian distribution that we love so much. We’ll start by reviewing the standard approach to regression using parametric models, but we will focus on a few particular details in order to set the scene for non-parametric models like Gaussian process regression. Next we will use these motivating ideas to define Gaussian Processes, and subsequently see how to cast the problem of regression in this form. We will then spend some time building an intuition for what information the regression formulas encode, and end by applying these techniques to modeling atmospheric CO2 concentration."
  },
  {
    "objectID": "posts/gp-regression/index.html#sec-parametric-prediction",
    "href": "posts/gp-regression/index.html#sec-parametric-prediction",
    "title": "Gaussian Process Regression",
    "section": "1.1 What Really Happens When We Predict on a New Point?",
    "text": "1.1 What Really Happens When We Predict on a New Point?\nGiven a new input point \\(x^*\\) we could obtain simply obtain \\(y^* = f_{\\theta^*}(x^*)\\) as our estimate by evaluating on the posterior maximizing function. A more principled approach would be to utilize the entire structure of the posterior, instead of just its maximum, because this allows us to compute \\[\nP(y^* | x^*, X, Y) = \\int P(y^* | \\theta, x^*) \\, P(\\theta | X, Y) \\,d\\theta \\approx P(y^* | \\theta^*, x^*) \\, P(\\theta^* | X, Y)\n\\]\nThe trick is that we can approximate the integral by its value at the maximum posterior, which is why we can simply use \\(\\theta^*\\) (this is rigorously justified as \\(N\\to\\infty\\) using the Saddle Point Approximation). This maybe feels a bit circular, but the point is that the object we really care about is \\(P(y^* | x^*, X, Y)\\): given the pattern of data we have seen before, and the location of the new input, what do we think the new output could be? Our parametric model allows us to approximate this quantity by picking a single set of parameter values: \\(\\theta^*\\). This means we can condition on the data without needing to look directly at it. We normally think about parameters as a tool for wrangling function space into a manageable form by making it low dimensional, but this is not all they do. The parameters \\(\\theta^*\\) act like convenient “buckets” for us to hold information about our data. By sacrificing full generality and restricting our functions to those accesible by our parameters, we are able to store a “summarized” version of our data in the values that the parameters take on. Gaussian Process Regression is a non-parametric model2. In that case when we compute \\(P(y^* | x^*, X, Y)\\), we will not have convenient “buckets” for us to store information about our data in, and instead we will need to store and look at all of our data whenever we predict on a new point. This is the price we will pay for the fllexibility of not being constrained to a parametric subspace of function space."
  },
  {
    "objectID": "posts/gp-regression/index.html#what-really-happens-when-we-put-a-prior-on-theta",
    "href": "posts/gp-regression/index.html#what-really-happens-when-we-put-a-prior-on-theta",
    "title": "Gaussian Process Regression",
    "section": "1.2 What Really Happens When we Put a Prior on \\(\\theta\\)?",
    "text": "1.2 What Really Happens When we Put a Prior on \\(\\theta\\)?\nLet’s suppose that we take mean 0 normal priors on both of the parameters of the linear model, so \\(a\\sim \\mathcal{N}(0, \\sigma_a^2)\\) and \\(b\\sim\\mathcal{N}(0, \\sigma_b^2)\\). We can clearly think about this as a probability distribution over the 2D space of parameters we have chosen, but there’s a sense in which this is a probability distribution over functions: every sample from the distribution produces an entire function for us. For example, in the figure below on the left we show some functions obtained by sampling parameters from their priors. It looks like a random collection of functions which we have sampled from a “probability distribution over functions”.\n\n\nCode\n# Randomly sample parameters of linear function from priors \ndef gen_random_lin_fn():\n    a, b = np.random.normal(size=2)\n    return lambda x: a*x + b\n\nxs = torch.arange(-2,2.1,0.1)\n\n# Plotting\nfig, ax = plt.subplots(1, 3, figsize=(15,5))\n\nfor i in range(10):\n    f = gen_random_lin_fn()\n    ax[0].plot(xs, f(xs), lw=5, alpha=0.75, c='#8da0cb')\n\nfn = gen_random_lin_fn()\nax[2].plot(xs, fn(xs), c='#8da0cb', lw=5)\nax[1].scatter(xs, fn(xs), c='#66c2a5', s=75)\n\n\n\n\n\n\n\n\nFigure 2: Left: Functions generated by sampling parameters from the prior on the linear regression model. Middle: Approximation of a function by evaluating it on a grid of points. Right: The same function in the continuum limit.\n\n\n\n\n\nCan we formalize this intuition? Can we express this probability density not in terms of the parameters, but in terms of the functions themselves, and sample functions directly rather than sample them indirectly via their parameters? One way to make this precise is to think about “sampling a function” as sampling the values that the function takes on when evaluated on its domain. For example we could approximate a function by considering the vector \\(\\vec{f} = (f(x_1), f(x_2), \\ldots, f(x_N))\\) evaluated at \\(N\\) points \\(x_i\\) evenly spaced between \\(\\left[-1, 1\\right]\\). As \\(N\\to\\infty\\) the vector \\(\\vec{f} \\to f(x)\\), shown in the middle and right respectively of Figure 2. We can then derive the “probability distribution over functions” by deriving the density for \\(\\vec{f}\\) from the prior over \\(\\theta\\). To do this for the linear model, we note that \\(\\vec{f} = a\\vec{x} + b\\vec{1}\\) where \\(\\vec{x} = (x_1, x_2, \\ldots x_N)\\) and \\(\\vec{1} = (1, 1, \\ldots, 1)\\). Since \\(a\\) and \\(b\\) are normally distributed and \\(\\vec{x}\\) and \\(\\vec{1}\\) are fixed, \\(\\vec{f}\\) is the sum of 2 vectors which are normally distributed in \\(\\mathbb{R}^N\\). In the appendix Section 7.1 we show that the sum of 2 normally distributed vectors must also be normally distributed. A normal distribution is completely characterized by its mean and covariance matrix, so we can derive the probability density for \\(\\vec{f}\\) by directly computing the expecation values \\(m_i \\equiv \\left&lt;f(x_i)\\right&gt;\\) and \\(K_{ij}\\equiv\\left&lt;(f(x_i) - m_i)(f(x_j)-m_j)\\right&gt;\\) with respect to \\(P(\\theta)\\). \\[\nm_i = \\left&lt;a \\right&gt;x_i + \\left&lt;b \\right&gt; = 0\n\\] Thus \\[\nK_{ij} = \\left&lt;f(x_i) f(x_j) \\right&gt; = \\left&lt;a^2 x_i x_j \\right&gt; + \\left&lt;ab(x_i + x_j) \\right&gt; + \\left&lt;b^2\\right&gt; = \\sigma_a^2 x_i x_j + \\sigma_b^2\n\\] Thus \\(\\vec{f}\\) is distributed as a multivariate normal in \\(\\mathbb{R}^N\\) with mean zero and covariance \\(K_{ij}\\equiv\\sigma_a^2 x_i x_j + \\sigma_b^2\\). \\[\n\\vec{f} \\sim \\mathcal{N}(\\vec{0}, K_{ij})\n\\tag{2}\\] Let’s visualize a few of these samples below on the left:\n\n\nCode\n# Initialize GP with linear kernel\nlin_GP = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        m=1,\n        log_c = torch.tensor(0.0),\n        log_variance= torch.tensor(0.0)\n    ),\n    log_noise = torch.tensor(0.0)\n)\n\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize=(10,4.5))\n\nfor i in range(5):\n    samp = lin_GP.sample(xs)\n    ax[0].scatter(xs, samp, c='#66c2a5', s=75)\n\n\nfor i in range(5):\n    xrand = torch.tensor(np.random.uniform(-2,2,size=len(xs)))\n    samp = lin_GP.sample(xrand)\n    ax[1].scatter(xrand, samp, c='#66c2a5', s=75)\n\n\n\n\n\n\n\n\nFigure 3: Left: Samples from our distribution over linear functions on an evenly spaced grid of points. Right: Samples from the same distribution for functions evaluated on a random sample of points.\n\n\n\n\n\nThey look like the samples we got from sampling the parameters directly. Great! A few comments are in order:\n\nOur probability distribution Equation 2 makes no reference the the parameters \\(a\\) and \\(b\\). It references the hyperparameters of their distributions \\(\\sigma_a\\) and \\(\\sigma_b\\), which makes sense since the structure of \\(P(\\theta)\\) must determine \\(P(\\vec{f})\\), but we are able to sample functions without any mention of the original parameters which defined these functions.\nWe define the kernel function \\(K(x, x') \\equiv \\sigma_a^2 x x' + \\sigma_b^2\\) as the function which takes 2 input points and returns the covariance with respect to the prior \\(\\left&lt;f(x)f(x')\\right&gt;\\) of the function evaluated at those two points. The kernel is doing the heavy lifting here. This is what defines our distribution, and encodes the fact that samples from it should be linear. As we get deeper into the subject, it will become apparent that the kernel is the star of the show.\nNothing in our derivation depended on the points \\(x_i\\) being evenly spaced. We could take \\(x_i\\) to be randomly sampled and the resultant \\(\\vec{f}\\) would still look linear, as shown in Figure 3 the right. We motivated using \\(\\vec{f}\\) as a proxy for \\(f(x)\\) by saying we would take the limit as \\(N\\to\\infty\\). This would take us to the land of functional integrals which physicists love, but statisticians not so much. The statisticians instead define the distribution over functions by demanding that for any collection of points \\(\\vec{f}\\sim\\mathcal{N}(\\vec{0}, K_{ij})\\) where the covariance is computed using the kernel on said collection of points.\nThe original parametric model confines us to a 2D slice of function space, but the distribution in Equation 2 works on all of function space: the whole dang infinite dimensional thing! If you took a function with non-linearity it would simply have 0 density. A non-rigorous way to see this is to observe that \\(K_{ij}\\) is rank 2 (it is the sum of 2 linearly independent rank 1 matrices) so \\(K^{-1}\\) is only defined for vectors on the span of the columns of \\(K_{ij}\\) which are linear functions. Non-linear functions are undefined under \\(K^{-1}\\) and so have 0 density in the Gaussian. Don’t worry if that’s hard to follow, the point is that we have a proper infinite dimensional distribution, the fact that our samples are linear is encoded by the kernel.\nDespite being elevated to a distribution on function space, there is still a clear sense in which our distribution is Gaussian. Every time we look at a finite sample of function values, they obey Gaussian statistics.\n\nThis, and the previous takeaway, may seem like rather odd perspectives on regression. The point is to show how seemingly foreign but central concepts from Gaussian process regression actually arise quite naturally from parametric regression. Let’s now turn to the real thing."
  },
  {
    "objectID": "posts/gp-regression/index.html#radial-basis-function-kernel",
    "href": "posts/gp-regression/index.html#radial-basis-function-kernel",
    "title": "Gaussian Process Regression",
    "section": "2.1 Radial Basis Function Kernel",
    "text": "2.1 Radial Basis Function Kernel\n\\[\\begin{equation}\nK(\\vec{x}_i, \\vec{x}_j) = \\sigma^2 \\exp\\left(-\\frac{||\\vec{x}_i - \\vec{x}_j||^2}{2 l^2}\\right)\n\\end{equation}\\] This is sometimes also referred to as the “RBF”, “Squared Exponential” or, “Gaussian” kernel. This kernel has the ability to sample from the entire infinite dimensional function space, and thus is a very powerful workhorse in the GP literature. Notice how the correlation drops with distance between input points. This has a “smoothing effect” in which nearby points are more correlated with each other. The hyper-parameters \\(\\sigma\\) and \\(l\\) control the scale of fluctuations about the mean, and the lengthscale over which you expect your samples to sustain correlations, and hence remain “smooth” over (I know I made a big deal earlier about GPs being non-parametric, but these are “hyper”-parameters which serve a very different role and we will say more about later). You can see this behavior in the plots below, where we show samples from GPs with an RBF kernels with different scales and variances.\n\n\nCode\n# Intialize 3 RBF GPs with different hyperparameter settings\nRBF_GP_1 = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_lengthscale=torch.log(torch.tensor(1.0)),\n        log_variance=torch.log(torch.tensor(1.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\nRBF_GP_2 = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_lengthscale=torch.log(torch.tensor(1.0)),\n        log_variance=torch.log(torch.tensor(5.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\nRBF_GP_3 = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_lengthscale=torch.log(torch.tensor(2.5)),\n        log_variance=torch.log(torch.tensor(1.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\n\nfig, ax = plt.subplots(1, 3, figsize=(15,4.5))\n\ncolors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']\nX = torch.arange(0,10,0.1)\n\nfor i in range(5):\n    y = RBF_GP_1.sample(X)\n    ax[0].plot(X, y, lw=4, c=colors[i])\n    y = RBF_GP_2.sample(X)\n    ax[1].plot(X, y, lw=4, c=colors[i])\n    y = RBF_GP_3.sample(X)\n    ax[2].plot(X, y, lw=4, c=colors[i])\n\nax[0].set_ylim([-5,5])\nax[1].set_ylim([-5,5])\nax[2].set_ylim([-5,5])\n\nax[0].set_title('$\\sigma^2 = 1.0$, $l=1.0$', fontsize=15);\nax[1].set_title('$\\sigma^2 = 5.0$, $l=1.0$', fontsize=15);\nax[2].set_title('$\\sigma^2 = 1.0$, $l=2.5$', fontsize=15);\n\n\n\n\n\n\n\n\nFigure 4: Samples from a GP with an RBF kernel with various hyperparameter settings."
  },
  {
    "objectID": "posts/gp-regression/index.html#white-noise-kernel",
    "href": "posts/gp-regression/index.html#white-noise-kernel",
    "title": "Gaussian Process Regression",
    "section": "2.2 White Noise Kernel",
    "text": "2.2 White Noise Kernel\n\\[\nK(\\vec{x}_i, \\vec{x}_j) = \\sigma^2 \\delta_{ij}\n\\] The Kronecker delta here symbolically just means the identity matrix. This kernel has no “smoothing”: function values are entirely uncorrelated regardless of how close they are to each other. This is not particularly useful for regression, as it places no useful structure on function space, but is instructive to understand how kernel structure gives rise to different classes of functions.\n\n\nCode\nwhite_GP_1 = GP.GaussianProcess(\n    kernel=GP.DeltaKernel(\n        log_variance=torch.log(torch.tensor(1.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\nwhite_GP_2 = GP.GaussianProcess(\n    kernel=GP.DeltaKernel(\n        log_variance=torch.log(torch.tensor(5.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\n\nfig, ax = plt.subplots(1, 2, figsize=(10,5))\n\ny = white_GP_1.sample(X)\nax[0].plot(X, y, lw=4, c='#8da0cb')\ny = white_GP_2.sample(X)\nax[1].plot(X, y, lw=4, c='#8da0cb')\n   \nax[0].set_ylim([-5,5])\nax[1].set_ylim([-5,5])\n\nax[0].set_title('$\\sigma^2 = 1.0$', fontsize=15);\nax[1].set_title('$\\sigma^2 = 5.0$', fontsize=15);\n\n\n\n\n\n\n\n\nFigure 5: White noise samples from the delta function kernel GP."
  },
  {
    "objectID": "posts/gp-regression/index.html#periodic-kernel",
    "href": "posts/gp-regression/index.html#periodic-kernel",
    "title": "Gaussian Process Regression",
    "section": "2.3 Periodic Kernel",
    "text": "2.3 Periodic Kernel\n\\[\nK(\\vec{x}_i, \\vec{x}_j) = \\sigma^2 \\exp\\left(- \\frac{2}{l^2}\\sin^2\\left(\\frac{||\\vec{x}_i - \\vec{x}_j||}{p}\\right)\\right)\n\\] Samples from this kernel are always periodic with period \\(p\\), and so is extremely useful for modeling data you know is periodic. (This kernel was derived by one of my personal heroes, the late David MacKay (MacKay 1998)). \\(\\sigma\\) and \\(l\\) play a similar role to that which they do in the RBF kernel, as you can see from the plots below.\n\n\nCode\nper_GP_1 = GP.GaussianProcess(\n    kernel=GP.PeriodicKernel(\n        log_lengthscale=torch.log(torch.tensor(1.0)),\n        log_variance=torch.log(torch.tensor(1.0)),\n        log_p = torch.log(torch.tensor(1.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\nper_GP_2 = GP.GaussianProcess(\n    kernel=GP.PeriodicKernel(\n        log_lengthscale=torch.log(torch.tensor(1.0)),\n        log_variance=torch.log(torch.tensor(1.0)),\n        log_p = torch.log(torch.tensor(3.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\nper_GP_3 = GP.GaussianProcess(\n    kernel=GP.PeriodicKernel(\n        log_lengthscale=torch.log(torch.tensor(2.5)),\n        log_variance=torch.log(torch.tensor(1.0)),\n        log_p = torch.log(torch.tensor(1.0))\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\n\nfig, ax = plt.subplots(1, 3, figsize=(15,5))\n\nfor i in range(5):\n    y = per_GP_1.sample(X)\n    ax[0].plot(X, y, lw=4, c=colors[i])\n    y = per_GP_2.sample(X)\n    ax[1].plot(X, y, lw=4, c=colors[i])\n    y = per_GP_3.sample(X)\n    ax[2].plot(X, y, lw=4, c=colors[i])\n\nax[0].set_ylim([-5,5])\nax[1].set_ylim([-5,5])\nax[2].set_ylim([-5,5])\n\nax[0].set_title('$\\sigma^2 = 1.0$, $l=1.0$, $p=1.0$', fontsize=15);\nax[1].set_title('$\\sigma^2 = 1.0$, $l=1.0$, $p=3.0$', fontsize=15);\nax[2].set_title('$\\sigma^2 = 1.0$, $l=2.5$, $p=1.0$', fontsize=15);\n\n\n\n\n\n\n\n\nFigure 6: Samples from the periodic kernel under different hyperparameter settings.\n\n\n\n\n\nA useful fact about kernels which I will not prove here, is that sums of kernels are kernels, and products of kernels are kernels (Gretton 2019). This means that existing kernels can be cobbled together to form exotic new kernels. For example we showed earlier that for linear models the kernel is given by \\[\nK_{lin}(\\vec{x}_i, \\vec{x}_j) = \\sigma^2(\\vec{x}_i\\cdot\\vec{x}_j + c)\n\\] with a slight redefinition of the hyperparameters. From this we can form the Polynomial Kernel: \\[\nK_{poly}(\\vec{x}_i, \\vec{x}_j) = \\sigma^2(\\vec{x}_i\\cdot\\vec{x}_j + c)^m\n\\] for integer \\(m\\). We know that this is a valid kernel because from the binomial expansion we can write this as a sum of powers of the linear kernel. Samples from this GP will generally produce degree \\(m\\) polynomials, with \\(\\sigma^2\\) and \\(c\\) controlling the polynomial weights and offset as seen below.\n\n\nCode\npoly_GP_1 = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        log_c=torch.log(torch.tensor(0.5)),\n        log_variance=torch.log(torch.tensor(1e-2)),\n        m = torch.tensor(2)\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\npoly_GP_2 = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        log_c=torch.log(torch.tensor(5.0)),\n        log_variance=torch.log(torch.tensor(1e-2)),\n        m = torch.tensor(2)\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\npoly_GP_3 = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        log_c=torch.log(torch.tensor(1.0)),\n        log_variance=torch.log(torch.tensor(1e-3)),\n        m = torch.tensor(4)\n    ),\n    log_noise=torch.log(torch.tensor(1.0))\n)\n\nfig, ax = plt.subplots(1, 3, figsize=(15,5))\n\nfor i in range(5):\n    y = poly_GP_1.sample(X)\n    ax[0].plot(X, y, lw=4, c=colors[i])\n    y = poly_GP_2.sample(X)\n    ax[1].plot(X, y, lw=4, c=colors[i])\n    y = poly_GP_3.sample(X)\n    ax[2].plot(X, y, lw=4, c=colors[i])\n   \nax[0].set_ylim([-5,5])\nax[1].set_ylim([-5,5])\nax[2].set_ylim([-5,5])\n\nax[0].set_title('$\\sigma^2 = 1e-2$, $m=2$, $c=0.5$', fontsize=15);\nax[1].set_title('$\\sigma^2 = 1e-2$, $m=2$, $c=5.0$', fontsize=15);\nax[2].set_title('$\\sigma^2 = 1e-3$, $m=4$, $c=1.0$', fontsize=15);\n\n\n\n\n\n\n\n\nFigure 7: Degree 2 (Left and Middle) and degree 4 (Right) samples from the polynomial kernel.\n\n\n\n\n\nThe kernels presented above barely scratch the surface of what’s out there. A good place to start exploring possible kernels is the “Kernel Cookbook” (Duvenaud 2014). Now that we formally know what a Gaussian Process is and have seen some real examples of them, let’s see how they can be used for regression."
  },
  {
    "objectID": "posts/gp-regression/index.html#posterior-calculation",
    "href": "posts/gp-regression/index.html#posterior-calculation",
    "title": "Gaussian Process Regression",
    "section": "3.1 Posterior Calculation",
    "text": "3.1 Posterior Calculation\nTo start, let’s generalize a bit and introduce some notation. Let \\(X\\) denote the collection of \\(N\\) training data points we have access to and \\(X^*\\) the collection of \\(M\\) test data points we would like to evaluate our regressing function on. As before we will take expressions like \\(f(X)\\) to mean the length \\(N\\) vector formed by evaluating \\(f\\) on the \\(N\\) elements of \\(X\\). Similarly, \\(K(X, X)\\) would be an \\(N\\times N\\) matrix. Finally we will take \\((f(X), f(X^*))\\) to mean the length \\(M+N\\) vector formed by concatenating \\(f(X)\\) and \\(f(X^*)\\), and similarly for matrices grouped in parentheses. To compute the conditional probability of \\(y^*\\), we will start by considering the joint distribution for \\((f(X), f(X^*))\\) under the assumption that \\(f\\) is drawn from a Gaussian Process3. By the definition of a GP we know this vector must have a normal distribution whose parameters are defined by the mean and kernel functions:\n\\[\n\\begin{pmatrix}\nf(X) \\\\\nf(X^*)\n\\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n    \\begin{pmatrix}\nm(X) \\\\\nm(X^*)\n\\end{pmatrix},\n\\begin{pmatrix}\nK(X, X)\\;\\; K(X, X^*)\\\\\nK(X^*, X)\\;\\; K(X^*, X^*)\n\\end{pmatrix}\n    \\right)\n\\] Recall however that we do not have direct access to \\(f(X)\\), but \\(Y = f(X) + \\epsilon\\) instead. Since \\(\\epsilon\\) is mean \\(0\\) it has no effect on the mean of \\((Y, f(X^*))\\) and we must simply compute its effect on the covariance. Denoting \\(\\hat{y_i}\\equiv y_i - m(x_i)\\) and \\(\\hat{f}(x_j^*)\\equiv f(x_j^*)-m(x_j^*)\\) we have \\[\\begin{multline}\n\\left&lt;\\hat{y}_i\\hat{y}_j\\right&gt; = \\left&lt;(\\hat{f}(x_i) + \\epsilon_i)(\\hat{f}(x_j) + \\epsilon_j)\\right&gt; = \\\\ \\left&lt;\\hat{f}(x_i)\\hat{f}(x_j)\\right&gt; + \\left&lt;\\epsilon_i\\right&gt;\\left&lt;\\hat{f}(x_j)\\right&gt; + \\left&lt;\\epsilon_j\\right&gt;\\left&lt;\\hat{f}(x_i)\\right&gt; + \\left&lt;\\epsilon_i\\epsilon_j\\right&gt; = K(x_i, x_j) + \\sigma_n^2 \\delta_{ij}\n\\end{multline}\\] where averages are taken over \\(f\\) and \\(\\epsilon\\), and we have used the fact that the \\(\\epsilon\\) are i.i.d. normally distributed with mean \\(0\\) and variance \\(\\sigma_n^2\\). Expanding \\(\\left&lt;\\hat{y}_i\\hat{f}(x_j)\\right&gt;\\) in a similar fashion produces no term which is quadratic in \\(\\epsilon\\), so the off diagonal blocks are unchanged, as is the lower right diagonal since it contains no \\(y\\) terms. Thus we have \\[\n\\begin{pmatrix}\nY \\\\\nf(X^*)\n\\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n    \\begin{pmatrix}\nm(X) \\\\\nm(X^*)\n\\end{pmatrix},\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I\\;\\; K(X, X^*)\\\\\nK(X^*, X)\\;\\; K(X^*, X^*)\n\\end{pmatrix}\n    \\right)\n\\tag{5}\\] where \\(I\\) denotes the identity matrix. For simplicity we define the \\(N\\times N\\) matrix \\(K_y\\equiv K(X, X) + \\sigma_n^2 I\\). In theory we could compute the conditional distribution on \\(f(X^*)\\) by fixing \\(Y\\) to it’s observed values, then multiplying by a normalizing factor. Things are slightly simplified by the fact that the conditional distribution of a gaussian is gaussian, so if we can massage the density into a form that looks like \\[\\begin{multline}\n-\\ln P(f(X^*) | X, Y) = \\frac{1}{2}(f(X^*) - \\mu_*)^T \\Sigma_*^{-1} (f(X^*) - \\mu_*) \\\\\n+ \\mathrm{constant\\; terms \\;independent \\;of \\;}f(X^*)\n\\end{multline}\\] We can read off the parameters \\(\\mu_*\\) and \\(\\Sigma_*\\) of the conditional distribution directly. The details of the calculation are quite cumbersome and we treat them in the appendix Section 7.2, but the basic idea is to notice that the log density of the joint distribution has the general form \\[\n\\frac{1}{2}f(X^*)^T A^{-1} f(X^*) + f(X^*)^T B^{-1} Y + \\frac{1}{2}Y^T C^{-1} Y\n\\] Where \\(A\\), \\(B\\), and \\(C\\) correspond to the block components of the \\(M+N\\) covariance matrix in Equation 5, and can be inverted using block matrix inversion. If \\(Y\\) is now a constant that we are conditioning on, this can be turned into a quadratic in \\(f\\) by “completing the square”. The calculation gives\n\n\n\n\n\n\n\\[\nf(X^*) | X, Y, \\theta \\sim \\mathcal{N}(\\mu_*, \\Sigma_*)\n\\] where \\(\\theta\\) denotes the collection of all noise and kernel hyperparameters and \\[\n\\mu_* = m(X^*) + K(X^*, X)\\left[K(X, X) + \\sigma_n^2 I\\right]^{-1}(Y - m(X))\n\\tag{6}\\] \\[\n\\Sigma_* = K(X^*, X^*) - K(X^*, X)\\left[K(X, X) + \\sigma_n^2 I\\right]^{-1}K(X, X^*)\n\\tag{7}\\]\n\n\n\nThe equations above are the entirety of what you need to do GP regression once you have chosen a kernel. One thing to note is how the noise variance \\(\\sigma_n^2\\) appears everytime a matrix is inverted. This stabilizes the inverse by putting a lower bound on the eigenvalues of \\(K_y\\), and is part of a deep connection between GP regression and kernel ridge regression. We’ll start by applying these formula to the miles per week dataset from the introduction, to get an intuitive feel for what these equations mean and how different kernels behave. Then we will break down the equations in detail and see if we can understand the mechanisms that produce this behavior.\n\n\nCode\nX_test = torch.arange(0,15.1,0.1, dtype=float)\n\n# Initialize RBF GP\nrun_RBF_GP_1 = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_variance = torch.log(torch.tensor(2.5)),\n        log_lengthscale = torch.log(torch.tensor(0.5))\n    ),\n    log_noise = torch.tensor(-torch.inf)\n)\n# Condition on train data to get parameters to compute posterior\nrun_RBF_GP_1.condition(t, miles)\n# Get posterior prediction on test data\nmu_1, var_1 = run_RBF_GP_1.predict(X_test, return_cov=True)\n\nrun_RBF_GP_2 = GP.GaussianProcess(\n    kernel=GP.RBFKernel(\n        log_variance = torch.log(torch.tensor(10.0)),\n        log_lengthscale = torch.log(torch.tensor(5.0))\n    ),\n    log_noise = torch.log(torch.tensor(1.0))\n)\nrun_RBF_GP_2.condition(t, miles)\nmu_2, var_2 = run_RBF_GP_2.predict(X_test, return_cov=True)\n\nrun_poly_GP = GP.GaussianProcess(\n    kernel=GP.PolynomialKernel(\n        m=2,\n        log_variance = torch.log(torch.tensor(1.0)),\n        log_c = torch.log(torch.tensor(1.0))\n    ),\n    log_noise = torch.log(torch.tensor(1.0))\n)\nrun_poly_GP.condition(t, miles)\nmu_3, var_3 = run_poly_GP.predict(X_test, return_cov=True)\n\nfig, ax = plt.subplots(1, 3, figsize=(15,5))\n\nax[0].scatter(t, miles, s=75, c='#66c2a5', zorder=2)\nax[0].plot(X_test, mu_1, lw=4, c='#8da0cb', zorder=1)\nax[0].fill_between(X_test, mu_1 - 3*torch.sqrt(torch.abs(var_1)), mu_1 + 3*torch.sqrt(torch.abs(var_1)), color='#e78ac3', alpha=0.5, zorder=0)\nax[0].set_title('RBF: $\\sigma^2_n = 0, \\sigma^2 = 2.5, l=0.5$');\n\nax[1].scatter(t, miles, s=75, c='#66c2a5', zorder=2)\nax[1].plot(X_test, mu_2, lw=4, c='#8da0cb', zorder=1)\nax[1].fill_between(X_test, mu_2 - 3*torch.sqrt(var_2), mu_2 + 3*torch.sqrt(var_2), color='#e78ac3', alpha=0.5, zorder=0)\nax[1].set_title('RBF: $\\sigma^2_n = 1, \\sigma^2 = 10, l=5$');\n\nax[2].scatter(t, miles, s=75, c='#66c2a5', zorder=2)\nax[2].plot(X_test, mu_3, lw=4, c='#8da0cb', zorder=1)\nax[2].fill_between(X_test, mu_3 - 3*torch.sqrt(var_3), mu_3 + 3*torch.sqrt(var_3), color='#e78ac3', alpha=0.5, zorder=0)\nax[2].set_title('Polynomial: $\\sigma^2_n = 1, m=2, \\sigma^2 = 1, c=1$');\n\n\nax[0].set_ylabel(\"Miles\", fontsize=15);\nax[0].set_xlabel(\"Time (Weeks)\", fontsize=15);\nax[1].set_xlabel(\"Time (Weeks)\", fontsize=15);\nax[2].set_xlabel(\"Time (Weeks)\", fontsize=15);\n\n\n\n\n\n\n\n\nFigure 8: GP regression posterior fits on the running mileage dataset. Left: A model with 0 observational noise perfectly interpolates the data. Middle: Allowing for observational noise and a larger RBF lengthscale gives a smoother fit with more reasonable generalization properties. Right: Regression with a polynomial kernel generalizes the trend with low uncertainty."
  },
  {
    "objectID": "posts/gp-regression/index.html#miles-per-week",
    "href": "posts/gp-regression/index.html#miles-per-week",
    "title": "Gaussian Process Regression",
    "section": "3.2 Miles per Week",
    "text": "3.2 Miles per Week\nWhen doing GP regression we treat \\(\\mu_*\\) as the best estimate for our regressing function, and use \\(\\Sigma_*\\) to tell us our uncertainty in that estimate at each point. In the first 2 plots of Figure 8 we show the results on our mileage dataset using an RBF kernel and \\(m=0\\) mean. In purple we plot \\(\\mu_*\\), and the pink bands show the 99 percent confidence band \\(\\pm3\\Sigma_*\\). Starting from the left, notice how when \\(\\sigma_n=0\\) we interpolate the data exactly and the model looks very overfit. (Strictly speaking interpolation is technically not synonymous with overfitting, but in practice zero-noise GPs with short lengthscales are a canonical example of overly flexible models) Also notice how the uncertainty is larger when further away from training data points. This is one of the great features of GP regression: we have adaptive uncertainty based on how far we are from the training data. Generalizing beyond the training data, the function quickly settles back to the prior mean. This is reasonable in the sense that the posterior has very little information when far away from the data, so it relies on the prior. It is bad in the sense that the regressed function doesn’t generalize the trend very well; the kernel we chose isn’t able to propagate information over long distances. In the middle we see a much more reasonable curve. The larger length scale in the kernel has a smoothing effect which damps the overfitting: the model is able to sustain correlations over a longer scale so the prediction at a given point is informed by more of the data, rather than just the training points nearby. Notice how even though the prior variance is higher, the uncertainty band is actually smaller. The uncertainty is also smooth near the data and slowly expands beyond the training data. This is a much more attractive form of adaptive uncertainty. The pattern on the left where uncertainty shrinks to \\(0\\) on the training data and balloons rapidly away from it is a clear sign of overfitting. This function still regresses to the mean when far enough away from the training data and is unable to propagate the observed trend over large distances. On the right we show a regression with a polynomial kernel with \\(m=2\\). This model has all of the previously mentioned nice properties, but is also able to sustain the trend over long range. This comes at the price: the kernel forces us into a low dimensional subspace of quadratic polynomials that must carry their trends for long distances. What GP regression gets us above Bayesian parametric regression on a quadratic model is the adaptive uncertainty estimation."
  },
  {
    "objectID": "posts/gp-regression/index.html#posterior-equations-in-detail",
    "href": "posts/gp-regression/index.html#posterior-equations-in-detail",
    "title": "Gaussian Process Regression",
    "section": "3.3 Posterior Equations in Detail",
    "text": "3.3 Posterior Equations in Detail\nLet’s take a deep dive into the posterior equations, and see if we can begin to understand the mechanisms that produce the qualitative behavior we saw above. We’ll consider a simpler (but typical) case of a mean \\(0\\) prior, and try to predict on a single point \\(x^*\\). Then \\[\n\\mu_*(x^*) = K(x^*, X)K_y^{-1}Y, \\;\\;\\;\\; \\Sigma_* = K(x^*, x^*) - K(x^*, X)K_y^{-1} K(X, x^*)\n\\] What happens when we evaluate on a training point, so \\(x^* = x_i\\), in the \\(\\sigma\\to 0\\) limit? Then \\(K_y^{-1} = K(X, X)^{-1}\\) so \\[\n\\mu_*(x_i) = \\sum_{j, k} K_{ij}K_{jk}^{-1}y_k = \\sum_{k}\\delta_{ik}y_k = y_i\n\\] and \\[\n[\\Sigma_*]_{ii} = K_{ii} - \\sum_{jk}K_{ij}K_{jk}^{-1}K_{ki} = 0\n\\] The regressing function exactly interpolates the training data with \\(0\\) variance at those points, as we saw above. Nice. What’s happening when we evaluate on test points off the training data? We define \\[\n\\alpha_i \\equiv \\sum_{j}[K_{y}^{-1}]_{ij} y_j\n\\] so that \\[\n\\mu_*(x^*) = \\sum_{i}\\alpha_i K(x^*, x_i)\n\\tag{8}\\] Each training point \\(x_i\\) computes a value \\(\\alpha_i\\) that it thinks is the correct output. We then use the kernel to measure how similar the test point is to each training point, and then take a weighted average of the \\(\\alpha_i\\) over all training points. The posterior uses the kernel to smoothly blend together estimates from different training points to form a prediction. Points which are more similar to \\(x^*\\) have their contribution weighted more highly, and vice-versa. Why take the \\(\\alpha_i\\) as each training point’s contribution instead of \\(y_i\\)? \\(y_i\\) is after all what the training point \\(x_i\\) thinks the output is. The problem is that the \\(y_i\\) can be strongly correlated with each other, as encoded by \\(K_y\\). What this means is that there is redundant information in the \\(y_i\\): if \\(y_i\\) contains information about the value of \\(y_j\\) and we try to use both for prediction then we will “double dip” and use overlapping information multipe times. \\(\\alpha_i\\) is a precision weighted version of \\(y_i\\): multiplying by \\(K_y^{-1}\\) removes the redundancy in the \\(y_i\\) so each \\(\\alpha_i\\) encodes the unique information carried by the output \\(y_i\\). This form also makes clear why our RBF kernel failed to generalize well: when the separation between \\(x^*\\) and \\(x_i\\) is \\(\\gg l\\), \\(K\\to 0\\) for the RBF kernel and \\(\\mu_*\\) falls back on the mean. The kernel is unable to propagate information over large distances."
  },
  {
    "objectID": "posts/gp-regression/index.html#the-price-of-non-parametrics",
    "href": "posts/gp-regression/index.html#the-price-of-non-parametrics",
    "title": "Gaussian Process Regression",
    "section": "3.4 The Price of Non-Parametrics",
    "text": "3.4 The Price of Non-Parametrics\nThe form of \\(\\mu_*\\) presented above offers a vivid interpretation of GP regression, and highlights a key feature of non-parametric methods. When discussing parametric regression we noted how by sacrificing flexibility, the parameters of these models acted as convenient containers to store information about our training data so that we can efficiently condition on them when making new predictions. GP regression pays the opposite price for the opposite power: in exchange for full reign over function space we must touch all of our data when we make a prediction on a new point. This is explicitly clear in the sum in Equation 8. In practice if you have an extremely large dataset, as one often does in deep learning, then constructing the weights \\(\\alpha_i\\) requires an extremely large matrix inversion that may seem inpractical (we discuss how this is handled in practice in appendix Section 7.3). It may also seem cumbersome to have to loop over the entire dataset everytime you run inference. And it is! It is important to keep in mind that there is no free lunch: the power and flexibility of GP regression comes at a price. If you have any experience with kernel machines the form of Equation 8 may look familiar. GP regression with observational noise is in fact exactly kernel ridge regression.4"
  },
  {
    "objectID": "posts/gp-regression/index.html#sec-app-A",
    "href": "posts/gp-regression/index.html#sec-app-A",
    "title": "Gaussian Process Regression",
    "section": "7.1 Appendix A: Sums of Gaussian Variables",
    "text": "7.1 Appendix A: Sums of Gaussian Variables\nHere we show if \\(\\vec{x}_1 \\sim \\mathcal{N}(\\vec{\\mu}_1, \\Sigma_1)\\) and \\(\\vec{x}_2 \\sim \\mathcal{N}(\\vec{\\mu}_2, \\Sigma_2)\\), then the random variable \\(\\vec{z} = \\vec{x}_1 + \\vec{x}_2 \\in \\mathbb{R}^d\\) is also normally distributed. We will rely on the fact that any affine transformation of a gaussian variable is still gaussian: for \\(\\vec{x}\\sim\\mathcal{N}(\\vec{\\mu}, \\Sigma)\\), then \\(A\\vec{x} + b\\sim\\mathcal{N}(A\\vec{\\mu} + b, A\\Sigma A^T)\\) for any matrix \\(A\\). This follows from the fact that if you transform a random variable, its density gets multiplied by the jacobian determinant of the transform. For an affine transformation the jacobian is independent of the variable itself so the density simply gets rescaled and preserves it’s functional Gaussian form. Consider the stacked vector \\[\n\\vec{w} =\n\\begin{pmatrix}\n\\vec{x}_1 \\\\\n\\vec{x}_2\n\\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n    \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix},\n\\begin{pmatrix}\n\\Sigma_1\\;\\; 0\\\\\n0\\;\\; \\Sigma_2\n\\end{pmatrix}\n    \\right)\n\\] Then for \\(A = \\left[I_d, I_d\\right] \\in \\mathbb{R}^{d\\times 2d}\\), we have \\(\\vec{z} = A \\vec{w}\\). Applying \\(A\\) to the stacked mean and covariance vectors and matrices gives: \\[\n\\vec{z} \\sim \\mathcal{N}(\\vec{\\mu}_1 + \\vec{\\mu}_2, \\Sigma_1 + \\Sigma_2)\n\\]"
  },
  {
    "objectID": "posts/gp-regression/index.html#sec-app-B",
    "href": "posts/gp-regression/index.html#sec-app-B",
    "title": "Gaussian Process Regression",
    "section": "7.2 Appendix B: Posterior Calculation",
    "text": "7.2 Appendix B: Posterior Calculation\nWe will complete the derivation assuming \\(m=0\\), the results for non-zero \\(m\\) carry through the same calculation straightforwardly. Starting from Equation 5 we introduce some notation: \\(A \\equiv K_y,\\;\\) \\(B \\equiv K(X, X^*),\\;\\) and \\(C\\equiv K(X^*, X^*)\\). Then the covariance matrix is given by \\[\n\\Sigma =\n\\begin{pmatrix}\nA\\;\\;\\;\\; B\\\\\nB^T\\;\\; C\n\\end{pmatrix}\n\\] Since \\(\\Sigma\\) is a block matrix we can apply the Schur complement formula to invert it. Defining \\(S = C - B^T A^{-1} B\\) then the inverse is \\[\n\\Sigma^{-1} =\n\\left(\n\\begin{array}{c c}\nA^{-1} + A^{-1} B S^{-1} B^T A^{-1}\n&\n- A^{-1} B S^{-1}\n\\\\[0.8ex]\n- S^{-1} B^T A^{-1}\n&\nS^{-1}\n\\end{array}\n\\right)\n\\] From this we can write the terms in the exponential of the density as \\[\n\\frac{1}{2}\n\\begin{pmatrix}\nY \\\\\nf(X^*)\n\\end{pmatrix}^T\n\\Sigma^{-1}\n\\begin{pmatrix}\nY \\\\\nf(X^*)\n\\end{pmatrix} = -\\frac{1}{2}\\left(f(X^*)^T S^{-1} f(X^*) - 2f(X^*)^T S^{-1} B^T A^{-1} y + \\ldots\\right)\n\\] Where the dots denote terms independent of \\(f(X^*)\\). We can form the conditional for \\(f(X^*)\\) by completing the square in \\(f(X^*)\\): \\[\n(f(X^*) - B^TA^{-1}Y)^T S^{-1}(f(X^*) - B^TA^{-1}Y) - Y^T A^{-1}BS^{-1}B^TA^{-1}Y\n\\] From this we can read off the conditional variance as \\(S\\) and mean as \\(B^T A^{-1} Y\\). Plugging in the definitions of \\(S, A, B, C\\) and re-inserting the mean \\(m\\) gives \\[\\begin{align*}\n\\mu_* = m(X^*) + K(X^*, X)\\left[K(X, X) + \\sigma_n^2 I\\right]^{-1}(Y - m(X)) \\\\\n\\Sigma_* = K(X^*, X^*) - K(X^*, X)\\left[K(X, X) + \\sigma_n^2 I\\right]^{-1}K(X, X^*)\n\\end{align*}\\] as we stated in the text."
  },
  {
    "objectID": "posts/gp-regression/index.html#sec-app-C",
    "href": "posts/gp-regression/index.html#sec-app-C",
    "title": "Gaussian Process Regression",
    "section": "7.3 Appendix C: Computing \\(K_y^{-1}\\)",
    "text": "7.3 Appendix C: Computing \\(K_y^{-1}\\)\nIn our posterior equations and hyperparameter optimization there are multiple points where we have to compute expressions of the form: \\[\nK_y^{-1} M = \\alpha\n\\] where M could be a matrix or vector. A related problem occurs in the hyperparameter optimization loss function where we have to compute \\(|K_y|\\). The positive semi-definite property of the kernel function means that in theory the matrix \\(K_y\\) is always invertible, but in practice the inverse matrix is usually numerically unstable due to floating point precision, especially in the presence of small eigenvalues. We instead make use of the fact that \\(K_y\\) is PSD so it admits a Cholesky Decomposition: \\[\nK_y = L L^T\n\\] Where \\(L\\) is a unique lower triangular that is guaranteed to exist. Solving for \\(L\\) is \\(O(n^3)\\), which is the same cost as inverting \\(K_y\\) directly, but is significantly more numerically stable. Once we have solved for \\(L\\) we can solve equations like \\(\\alpha = K_y^{-1}M\\) without ever actually forming \\(K_y^{-1}\\) by instead solving linear systems of equations, which itself is also much more stable. In detail, we first solve \\[\nL z = M\n\\] followed by \\[\nL^T \\alpha = z\n\\] each of these solves is \\(O(n^2)\\) due to the lower triangular structure of \\(L\\). A further property we exploit is that the Cholesky factors allow for an efficient computation of the determinant of \\(K_y\\): \\[\n\\ln |K_y| = 2\\sum_{i} \\ln L_{ii}\n\\] All inverse and determinant calculations done when solving for the posterior means, covariances, or hyperparameter loss functions are done according to this framework. In particular, the gradient of the hyperparameter loss must also be computed in the same manner, but if you use auto-diff software such as torch you must take care to construct the loss function with the Cholesky decomposition so that the gradients backpropagate through \\(L\\) property."
  },
  {
    "objectID": "posts/gp-regression/index.html#sec-app-D",
    "href": "posts/gp-regression/index.html#sec-app-D",
    "title": "Gaussian Process Regression",
    "section": "7.4 Appendix D: Hyperparameter Loss Gradient",
    "text": "7.4 Appendix D: Hyperparameter Loss Gradient\nStarting from Equation 9, we let \\(\\theta\\) denote the set of all hyperparameters including the noise variance \\(\\sigma^2_n\\): \\[\n\\mathcal{L}(\\theta) = -\\frac{1}{2}Y^T K_y^{-1}Y - \\frac{1}{2} \\ln |K_y|\n\\] We will compute the gradient of each term separately. In order to take the derivative of \\(K_y^{-1}\\) we note that \\(\\mathbb{I} = K_y^{-1}K_y\\), and the derivative of \\(\\mathbb{I}\\) is 0 so \\[\n0 = \\frac{\\partial K_y^{-1}}{\\partial \\theta_i}K_y + K_y^{-1}\\frac{\\partial K_y}{\\partial \\theta_i} \\implies \\frac{\\partial K_y^{-1}}{\\partial \\theta_i} = - K_y^{-1}\\frac{\\partial K_y}{\\partial \\theta_i} K_y^{-1}\n\\] So the derivative of the first term gives \\[\n\\frac{1}{2}Y^T K_y^{-1}\\frac{\\partial K_y}{\\partial \\theta_i} K_y^{-1} Y = \\frac{1}{2}\\alpha^T\\frac{\\partial K_y}{\\partial \\theta_i}\\alpha\n\\] Where we have defined \\(\\alpha\\equiv K_y^{-1} Y\\) and made use of the fact the \\(K_y^{-1}\\) is symmetric. For the second term we denote the eigenvalues of \\(K_y\\) by \\(\\lambda_i\\), so \\[\n|K_y| = \\prod_i \\lambda_i = e^{\\sum_i \\ln \\lambda_i} = e^{\\mathrm{Tr} \\ln K_y}\n\\] Since the log of a diagonalized matrix is just the log of its elements. Thus \\[\n\\frac{\\partial \\ln |K_y|}{\\partial \\theta_i} = \\mathrm{Tr} \\left(K_y^{-1}\\frac{\\partial K_y}{\\partial \\theta_i}\\right)\n\\] At this point we could the terms for the derivative together, but we can massage the first term into the form of a trace. For arbitrary matrix \\(A\\) we have \\[\n\\alpha^T A \\alpha = \\sum_{ij}\\alpha_i A_{ij}\\alpha_j = \\sum_j \\left(\\sum_i \\alpha_j \\alpha_i A_{ij}\\right) = \\sum_j (\\alpha\\alpha^T A)_{jj} = \\mathrm{Tr}(\\alpha\\alpha^T A)\n\\] This allows us to combine both derivative terms into a single trace: \\[\n\\frac{\\partial\\mathcal{L}}{\\partial \\theta_i} = \\frac{1}{2}\\mathrm{Tr}\\left[\\left(\\alpha\\alpha^T - K_y^{-1}\\right)\\frac{\\partial K_y^{-1}}{\\partial \\theta_i}\\right]\n\\]"
  },
  {
    "objectID": "posts/gp-regression/index.html#footnotes",
    "href": "posts/gp-regression/index.html#footnotes",
    "title": "Gaussian Process Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am assuming you have seen this line of reasoning before and so am glossing over the details↩︎\nStrictly speaking, non-parametric models don’t have no parameters, but rather the number of parameters grows with the amount of data we have.↩︎\nThis calculation may not look very “Bayesian” in the sense that we never use a formula like Equation 1 to obtain the posterior. The calculation can in fact be done in this manner, but requires some heavy machinery from the world of Reproducing Kernel Hilbert Spaces. In order not to open up this can of worms here we opt for the more direct but less Bayesian looking approach.↩︎\nTo treat the fascinating connection between GP regression and kernel machines in a manner that would satisfy me would explode the length of this already long post. It is a subject that I hope to return to in a future post so stay tuned!↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Gaussian Process Regression\nDecember 31, 2025 Distributions over functions, kernel design, and uncertainty in regression."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Not Entirely Unambiguous",
    "section": "",
    "text": "A blog at the convex hull of ML, Physics, and Bayesian Inference.\n\nHi - I’m Anoop.\nI’m an AI researcher at Unlearn.ai with a background in Physics. I’m broadly interested in Bayesian Inference, Statistical Physics, and Interpretability. This site is a collection of technical notes and essays on topics that interest me. I hope they interest you too!\n\n\nRecent Posts\n\n\n\n\n\n\n\nGaussian Process Regression\nDecember 31, 2025\nDistributions over functions, kernel design, and uncertainty in regression.\n\n\n\n\n\n\nView all posts →"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "1 Contact",
    "text": "1 Contact\n\nanooppraturu@gmail.com"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Not Entirely Unambiguous",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\nGaussian Process Regression\nDecember 29, 2025\nDistributions over functions, kernel design, and uncertainty in regression.\n\n\n\n\n\n\nView all posts →"
  }
]